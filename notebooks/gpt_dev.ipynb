{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqSIiWsRKdi7"
      },
      "source": [
        "# Configure MLflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdco6DJEr1Hs"
      },
      "source": [
        "import requests\n",
        "from getpass import getpass\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScoLtmkOSTLE"
      },
      "source": [
        "**Set Environment Variables**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEOF1GMt9pIa"
      },
      "source": [
        "**Initialize MLflow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKazlYv0rKoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae1040b-200d-491e-fc4a-af5a7eb77266"
      },
      "source": [
        "!pip install mlflow --quiet\n",
        "\n",
        "import mlflow\n",
        "import os"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVIOWPHaaX8I"
      },
      "source": [
        "**Set Local Configurations**\n",
        "\n",
        "Under the [Token tab](https://dagshub.com/user/settings/tokens) in the user setting, copy the default token and use it here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg5u4G9tvL_d"
      },
      "source": [
        "### Source\n",
        "- https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "- https://huyenchip.com/2023/05/02/rlhf.html#language_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7c3I6TCoNbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98a76d8-ce9d-4fa9-b16c-14c15ca175bb"
      },
      "source": [
        "# REPO_OWNER = input('Repo owner: ').strip()\n",
        "# REPO_NAME = input('Repo name: ').strip()\n",
        "\n",
        "\n",
        "REPO_OWNER = 'lnnchau'\n",
        "REPO_NAME = 'mlops-lyrics-lm'\n",
        "USER_NAME = REPO_OWNER\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = USER_NAME\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token: ')\n",
        "\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/{REPO_OWNER}/{REPO_NAME}.mlflow')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your DAGsHub access token: 路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow"
      ],
      "metadata": {
        "id": "zxBm7vZXMy09"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = mlflow.pytorch.load_model('models:/BigramLanguageModel/5', map_location='cpu')\n",
        "import torch\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "\n",
        "\n",
        "model(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLH9B065NQbG",
        "outputId": "d83ba294-506b-4e1e-909e-7074566da74d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 3.1897, -0.1859, -2.3211, -1.6164, -3.7888, -1.3246, -2.0342,\n",
              "           -0.1595, -3.3363, -0.2134,  0.7939, -3.1611, -3.9309, -4.2971,\n",
              "           -2.1758, -2.5180,  4.0481,  2.7661, -4.5270, -2.5663, -0.1953,\n",
              "           -1.5023,  2.7343,  0.6043,  3.5420, -2.7502,  2.0626, -1.0538,\n",
              "            0.6115, -1.6857, -3.4115, -2.4753, -3.5058, -3.4910, -3.0509,\n",
              "           -4.1252,  0.2435, -1.4038, -0.8380, -1.8344,  0.7848, -0.7693,\n",
              "           -0.2874, -2.8874,  3.3461, -4.0209, -1.3799, -1.3985, -0.0450,\n",
              "           -0.8859, -2.1166]]], grad_fn=<ViewBackward0>),\n",
              " None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r373yxYEo0Yu"
      },
      "source": [
        "## Language model\n",
        "- GPT is a LLM (large language models)\n",
        "- Language model encodes the statistical information of the language. It tells you what's likely to appear in a context.\n",
        "    - Example: (find 2 examples predicting next word / fill in the blanks)\n",
        "    - To train a language model, you feed it a lot of text (training data) so that it can learn the statistical information from it.\n",
        "- Word-level vs Character-level?\n",
        "- This notebook: Character-level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvp_S2yHuomF"
      },
      "source": [
        "\n",
        "## n-gram model\n",
        "### What this model do?\n",
        "- Predict how likely the next word is, given n-1 preceding words in the sequence.\n",
        "- For example,\n",
        "    - n=2 (bigram model)\n",
        "    - assume our dictionary has the following words: to, I, movies, like, watch, you, we, books\n",
        "    - input: `I like to watch ____`\n",
        "    - context sequence: `watch`\n",
        "    - what the bigram does, is that, it'll go over all words in the dictionary, and compute how likely it is the next word - more formally, `P(word_i|watch)`\n",
        "\n",
        "### How to train this model?\n",
        "yadayadayada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFP_M0VwKPm"
      },
      "source": [
        "#### Build the vocabulary\n",
        "- Some concepts in NLP\n",
        "- `Document`: text objects, which could be an article, a movie review, a passage or even a sentence.\n",
        "- `Corpus`: list of documents\n",
        "- `Vocabulary`: list of all the tokens in all documents. based on the task, token could be either a word, a character, or parts of the word (e.g. `playing` can be split into two tokens `play` and `ing`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fNfQe70syaaz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QgQEFj9LnSMM",
        "outputId": "9b43371e-4da7-4b30-bb79-b60cd62df136"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  artist                   song                                        link  \\\n",
              "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
              "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
              "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
              "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
              "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
              "\n",
              "                                                text  \n",
              "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
              "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
              "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
              "3  Making somebody happy is a question of give an...  \n",
              "4  Making somebody happy is a question of give an...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf857a59-0e75-4b39-b226-34a0870cd867\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Ahe's My Kind Of Girl</td>\n",
              "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
              "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Andante, Andante</td>\n",
              "      <td>/a/abba/andante+andante_20002708.html</td>\n",
              "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>As Good As New</td>\n",
              "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
              "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang</td>\n",
              "      <td>/a/abba/bang_20598415.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang-A-Boomerang</td>\n",
              "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf857a59-0e75-4b39-b226-34a0870cd867')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf857a59-0e75-4b39-b226-34a0870cd867 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf857a59-0e75-4b39-b226-34a0870cd867');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/spotify_millsongdata.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGzyejwYybYA",
        "outputId": "f1cdbd2e-db83-4a1b-bc72-152456c5f6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus has 57650 documents\n",
            "Sample document: look at her face, it's a wonderful face  \r\n",
            "and it means something special to me  \r\n",
            "look at the way that she smiles when she sees me  \r\n",
            "how lucky can one fellow be?  \r\n",
            "  \r\n",
            "she's just my kind of girl, she makes me feel fine  \r\n",
            "who could ever believe that she could be mine?  \r\n",
            "she's just my kind of girl, without her i'm blue  \r\n",
            "and if she ever leaves me what could i do, what could i do?  \r\n",
            "  \r\n",
            "and when we go for a walk in the park  \r\n",
            "and she holds me and squeezes my hand  \r\n",
            "we'll go on walking for hours and talking  \r\n",
            "about all the things that we plan  \r\n",
            "  \r\n",
            "she's just my kind of girl, she makes me feel fine  \r\n",
            "who could ever believe that she could be mine?  \r\n",
            "she's just my kind of girl, without her i'm blue  \r\n",
            "and if she ever leaves me what could i do, what could i do?\r\n",
            "\r\n",
            "\n"
          ]
        }
      ],
      "source": [
        "corpus = data.text.str.lower()\n",
        "sample_document = corpus[0]\n",
        "\n",
        "print(f'Corpus has {len(corpus)} documents')\n",
        "print(f'Sample document: {sample_document}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SneDZpVyu0u",
        "outputId": "2241197e-fee1-4199-d2c3-399c134c1f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocab: 51\n",
            "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "corpus_as_string = ' '.join(corpus.values)\n",
        "vocab = set(corpus_as_string)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'Length of vocab: {vocab_size}')\n",
        "print(sorted(list(vocab)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        # create a mapping from characters to integers\n",
        "        self.stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "\n",
        "    def encode(self, s):\n",
        "        # encoder: take a string, output a list of integers\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, l):\n",
        "        # decoder: take a list of integers, output a string\n",
        "        return ''.join([self.itos[i] for i in l])\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "print(tokenizer.encode(\"hii there\"))\n",
        "print(tokenizer.decode(tokenizer.encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQtWLjeY1xzw",
        "outputId": "c9fd3197-08da-4ec8-ed68-209227223bc8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 48, 48, 40, 41, 3, 43, 16, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRbFPwOh0AN7",
        "outputId": "3641bb93-71df-49ee-d45d-d16296577922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 48, 48, 40, 41, 3, 43, 16, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "ca2836e0-ac52-4500-dc75-d25694ba8ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([70426172]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(corpus_as_string), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "# print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EorJrz254GuS"
      },
      "source": [
        "#### Create dataset\n",
        "- train_size: 90%\n",
        "- val_size: 105\n",
        "- seq_len (block size for now): 8\n",
        "    - what is the maximum context length for predictions?\n",
        "- batch_size = 4 # how many independent sequences will we process in parallel?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.8*len(data)) # first 90% will be train, rest val\n",
        "\n",
        "train_ratio, val_ratio, test_ratio = 0.9, 0.05, 0.05\n",
        "\n",
        "train_size = int(train_ratio * len(data))\n",
        "val_size = int(val_ratio * len(data))\n",
        "test_size = int(test_ratio * len(data))\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size+val_size]\n",
        "test_data = data[-test_size:]\n",
        "\n",
        "assert len(train_data) == train_size\n",
        "assert len(val_data) == val_size\n",
        "assert len(test_data) == test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "c41d5b8d-4560-4157-eb0e-ee12692fa11c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15, 37, 37, 47, 40, 13, 41, 40,  3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "c14d69fc-ed74-45c7-9d59-90d6b88a6cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([15]) the target: 37\n",
            "when input is tensor([15, 37]) the target: 37\n",
            "when input is tensor([15, 37, 37]) the target: 47\n",
            "when input is tensor([15, 37, 37, 47]) the target: 40\n",
            "when input is tensor([15, 37, 37, 47, 40]) the target: 13\n",
            "when input is tensor([15, 37, 37, 47, 40, 13]) the target: 41\n",
            "when input is tensor([15, 37, 37, 47, 40, 13, 41]) the target: 40\n",
            "when input is tensor([15, 37, 37, 47, 40, 13, 41, 40]) the target: 3\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "c8385988-c6a2-48ba-cd8f-f18638324232"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[27, 43, 36, 40, 28, 37, 41, 40],\n",
              "         [29, 40, 30, 13, 28, 48, 27, 40],\n",
              "         [43, 40, 23, 13, 15, 15, 29, 40],\n",
              "         [40, 18, 37, 34, 16, 40, 13, 16]]),\n",
              " tensor([[43, 36, 40, 28, 37, 41, 40, 41],\n",
              "         [40, 30, 13, 28, 48, 27, 40, 20],\n",
              "         [40, 23, 13, 15, 15, 29, 40, 40],\n",
              "         [18, 37, 34, 16, 40, 13, 16, 30]]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split, batch_size, block_size):\n",
        "    '''\n",
        "    Generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    return: x, y\n",
        "        - x: (batch_size, block_size)\n",
        "        - y: (batch_size, block_size)\n",
        "    '''\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "get_batch('train', 4, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "bBBlDCWQW3uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interface\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1JXLNzdWehE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Config`\n",
        "Store parameters settings"
      ],
      "metadata": {
        "id": "-3C28MvmXbcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self,\n",
        "                 batch_size,\n",
        "                 max_iterations,\n",
        "                 eval_iterations,\n",
        "                 eval_intervals,\n",
        "                 lr,\n",
        "                 vocab_size,\n",
        "                 block_size,\n",
        "                 n_embd,\n",
        "                 n_head,\n",
        "                 n_layer,\n",
        "                 dropout):\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iterations = max_iterations\n",
        "        self.eval_iterations = eval_iterations\n",
        "        self.eval_intervals = eval_intervals\n",
        "\n",
        "        self.lr = lr\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.n_embd = n_embd\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "2enOPQatWd_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML architectures\n",
        "Should receive `Config` object as param to construction method\n"
      ],
      "metadata": {
        "id": "TdCBC4J-XX6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 1: Simple BigramLanguageModel"
      ],
      "metadata": {
        "id": "j0EA_ByeJQO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nql_1ER53oCf"
      },
      "outputs": [],
      "source": [
        "from IPython.core.formatters import Configurable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, config: Configurable):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        vocab_size = config.vocab_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (batch_size,seq_len) tensor of integers\n",
        "\n",
        "        logits = self.token_embedding_table(idx) # (batch_size,seq_len,vocab_size)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "            # example: we have 2 classes [0, 1]\n",
        "            # logits = [[0.5, 0.5], [0.3, 0.7], [0.6, 0.4]]\n",
        "            # targets = [0, 1, 0]\n",
        "\n",
        "            logits = logits.view(batch_size*seq_len, vocab_size)\n",
        "            targets = targets.view(batch_size*seq_len)\n",
        "  \n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (batch_size, vocab_size)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_ppl(self, seq_tensor):\n",
        "        # Implemented from https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "        logits, _ = self(seq_tensor)    # logits = (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        logits = logits.view(batch_size*seq_len, vocab_size)\n",
        "        \n",
        "        logits = logits[:-1, :] # to compute P(x_i|x_(i-1))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        \n",
        "        ground_truths = seq_tensor.view(batch_size*seq_len)[1:]\n",
        "        ppl = probs[\n",
        "            torch.arange(batch_size*seq_len - 1),\n",
        "            ground_truths]\n",
        "\n",
        "        return torch.exp(ppl.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 2: Bigram Language Model with Attention"
      ],
      "metadata": {
        "id": "BN4CZhZ_WSTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, config: Config):\n",
        "        super().__init__()\n",
        "\n",
        "        n_embd = config.n_embd\n",
        "        block_size = config.block_size\n",
        "        dropout = config.dropout\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "    \n",
        "        n_head = config.n_head\n",
        "        n_embd = config.n_embd\n",
        "        dropout = config.dropout\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size, config) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "\n",
        "        n_embd = config.n_embd\n",
        "        n_head = config.n_head\n",
        "        dropout = config.dropout\n",
        "        \n",
        "        self.sa = MultiHeadAttention(config)\n",
        "        self.ffwd = FeedFoward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModelAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_ppl(self, seq_tensor):\n",
        "        _, test_size = seq_tensor.shape\n",
        "        num_chunks = test_size // block_size\n",
        "\n",
        "        # update test size to fit with block_size\n",
        "        test_size = num_chunks * block_size\n",
        "        seq_tensor = seq_tensor[:, :test_size]\n",
        "\n",
        "        # split seq_tensor into chunks of blocksize\n",
        "        test_input = seq_tensor.view(num_chunks, block_size)\n",
        "\n",
        "        # Implemented from https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "        logits, _ = self(test_input)    # logits = (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        logits = logits.view(test_size, vocab_size)\n",
        "        logits = logits[:-1, :] # to compute P(x_i|x_(i-1))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        \n",
        "        # TODO: write docs to explain this part\n",
        "        ground_truths = test_input.view(test_size)[1:]\n",
        "        ppl = probs[\n",
        "            torch.arange(test_size - 1),\n",
        "            ground_truths]\n",
        "\n",
        "        return torch.exp(ppl.mean())\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "ayuMVxMw2U5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `LyricsGenerator`\n",
        "Wrapper for inference - should receive `model` and `tokenizer` as params to contruction method"
      ],
      "metadata": {
        "id": "vjI5J2HTXv_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LyricsGenerator:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def get_lyrics(self, start_phrase, max_new_tokens=2000):\n",
        "        start_phrase_as_ids = self.tokenizer.encode(start_phrase)\n",
        "        context = torch.tensor(start_phrase_as_ids, dtype=torch.long, device=self.device).reshape(1, -1)\n",
        "        output_tokens = self.model.generate(idx=context, max_new_tokens=max_new_tokens)[0].tolist()\n",
        "\n",
        "        return self.tokenizer.decode(output_tokens)"
      ],
      "metadata": {
        "id": "X8CyxCblHiHn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Trainer`\n",
        "Encapsulates training and evaluate code"
      ],
      "metadata": {
        "id": "tbRL4MzAX1Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "qe41CMO5amaq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model_class, config, get_batch):\n",
        "        self.config = config\n",
        "        self.model_class = model_class\n",
        "        self.model = model_class(config)\n",
        "        self.get_batch = get_batch\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def estimate_loss(self):\n",
        "        model = self.model\n",
        "        config = self.config\n",
        "\n",
        "        out = {}\n",
        "        model.eval()\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(config.eval_iterations)\n",
        "            for k in range(config.eval_iterations):\n",
        "                X, Y = self.get_batch(split, config.batch_size, config.block_size)\n",
        "                X, Y = X.to(config.device), Y.to(config.device)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "        model.train()\n",
        "        return out\n",
        "\n",
        "    def evaluate_test(self, test_data):\n",
        "        device = self.config.device\n",
        "        model = self.model\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        model = model.to(device)\n",
        "        test_data = test_data.to(device)\n",
        "        ppl = model.evaluate_ppl(test_data.view(1, -1))\n",
        "\n",
        "        mlflow.log_metric('ppl', ppl)\n",
        "        return ppl\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        model_class = self.model_class\n",
        "        config = self.config\n",
        "\n",
        "        mlflow.set_experiment_tag('architecture', model_class.__name__)\n",
        "        mlflow.log_params(config.__dict__)\n",
        "\n",
        "        m = self.model\n",
        "        m.to(config.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(m.parameters(), lr=config.lr)\n",
        "\n",
        "        m.train()\n",
        "\n",
        "        for step in range(config.max_iterations):\n",
        "            if step % config.eval_intervals == 0 or step == config.max_iterations - 1:\n",
        "                losses = self.estimate_loss()\n",
        "                print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "                mlflow.log_metrics(losses, step=step)\n",
        "\n",
        "            # sample a batch of data\n",
        "            xb, yb = self.get_batch(\n",
        "                'train',\n",
        "                batch_size=config.batch_size,\n",
        "                block_size=config.block_size)\n",
        "            xb, yb = xb.to(config.device), yb.to(config.device)\n",
        "\n",
        "            # evaluate the loss\n",
        "            logits, loss = m(xb, yb)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        m.to(\"cpu\")\n",
        "        mlflow.pytorch.log_model(m, \"model\")\n",
        "\n",
        "        return m"
      ],
      "metadata": {
        "id": "McSaeCw_YXf9"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run experiments"
      ],
      "metadata": {
        "id": "DHE2IyAGb-0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import time"
      ],
      "metadata": {
        "id": "GdeTMhQj3pr6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(\n",
        "    batch_size = 16, # how many independent sequences will we process in parallel?\n",
        "    block_size = 32, # what is the maximum context length for predictions?\n",
        "    max_iterations = 10000,\n",
        "    eval_intervals = 100,\n",
        "    lr = 1e-3,\n",
        "    eval_iterations = 200,\n",
        "    n_embd = 64,\n",
        "    n_head = 4,\n",
        "    n_layer = 4,\n",
        "    dropout = 0.0,\n",
        "    vocab_size=vocab_size,\n",
        ")"
      ],
      "metadata": {
        "id": "vtacs8JxLUuS"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FXfIK_pJSLta"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = data[-test_size:]\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "tokenizer_path = 'tokenizer.pkl'\n",
        "\n",
        "with open(tokenizer_path, 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "trainer = Trainer(\n",
        "    BigramLanguageModelAttention,\n",
        "    config=config,\n",
        "    get_batch=get_batch\n",
        ")\n",
        "\n",
        "with mlflow.start_run(run_name=str(time.time())):\n",
        "    trainer.train()\n",
        "    trainer.evaluate_test(test_data)\n",
        "\n",
        "    mlflow.log_artifact(tokenizer_path, \"model/artifacts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtwMJ5uEQDUl",
        "outputId": "0deab2e5-1546-4914-a3ae-55a79b979bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1192, val loss 4.1194\n",
            "step 100: train loss 2.4006, val loss 2.3954\n",
            "step 200: train loss 2.2343, val loss 2.2241\n",
            "step 300: train loss 2.1417, val loss 2.1217\n",
            "step 400: train loss 2.0808, val loss 2.0735\n",
            "step 500: train loss 2.0126, val loss 2.0015\n",
            "step 600: train loss 1.9557, val loss 1.9365\n",
            "step 700: train loss 1.9240, val loss 1.9025\n",
            "step 800: train loss 1.8797, val loss 1.8709\n",
            "step 900: train loss 1.8415, val loss 1.8349\n",
            "step 1000: train loss 1.8033, val loss 1.7939\n",
            "step 1100: train loss 1.7751, val loss 1.7766\n",
            "step 1200: train loss 1.7570, val loss 1.7446\n",
            "step 1300: train loss 1.7381, val loss 1.7135\n",
            "step 1400: train loss 1.7216, val loss 1.7103\n",
            "step 1500: train loss 1.7069, val loss 1.6974\n",
            "step 1600: train loss 1.7052, val loss 1.6971\n",
            "step 1700: train loss 1.6739, val loss 1.6619\n",
            "step 1800: train loss 1.6763, val loss 1.6614\n",
            "step 1900: train loss 1.6522, val loss 1.6399\n",
            "step 2000: train loss 1.6462, val loss 1.6408\n",
            "step 2100: train loss 1.6384, val loss 1.6234\n",
            "step 2200: train loss 1.6385, val loss 1.6174\n",
            "step 2300: train loss 1.6202, val loss 1.6081\n",
            "step 2400: train loss 1.6180, val loss 1.6209\n",
            "step 2500: train loss 1.6022, val loss 1.5904\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11v-z8aSyolKHJBarefSI1rZO_KGSlOCL",
      "authorship_tag": "ABX9TyOEHzIDhhkiYiRSLMKYcq1O"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}