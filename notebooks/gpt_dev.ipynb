{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqSIiWsRKdi7"
      },
      "source": [
        "# Configure MLflow 🧐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdco6DJEr1Hs"
      },
      "source": [
        "import requests\n",
        "from getpass import getpass\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScoLtmkOSTLE"
      },
      "source": [
        "**Set Environment Variables**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEOF1GMt9pIa"
      },
      "source": [
        "**Initialize MLflow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKazlYv0rKoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae1040b-200d-491e-fc4a-af5a7eb77266"
      },
      "source": [
        "!pip install mlflow --quiet\n",
        "\n",
        "import mlflow\n",
        "import os"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVIOWPHaaX8I"
      },
      "source": [
        "**Set Local Configurations**\n",
        "\n",
        "Under the [Token tab](https://dagshub.com/user/settings/tokens) in the user setting, copy the default token and use it here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg5u4G9tvL_d"
      },
      "source": [
        "### Source\n",
        "- https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "- https://huyenchip.com/2023/05/02/rlhf.html#language_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7c3I6TCoNbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98a76d8-ce9d-4fa9-b16c-14c15ca175bb"
      },
      "source": [
        "# REPO_OWNER = input('Repo owner: ').strip()\n",
        "# REPO_NAME = input('Repo name: ').strip()\n",
        "\n",
        "\n",
        "REPO_OWNER = 'lnnchau'\n",
        "REPO_NAME = 'mlops-lyrics-lm'\n",
        "USER_NAME = REPO_OWNER\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = USER_NAME\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token: ')\n",
        "\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/{REPO_OWNER}/{REPO_NAME}.mlflow')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your DAGsHub access token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow"
      ],
      "metadata": {
        "id": "zxBm7vZXMy09"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = mlflow.pytorch.load_model('models:/BigramLanguageModel/5', map_location='cpu')\n",
        "import torch\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "\n",
        "\n",
        "model(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLH9B065NQbG",
        "outputId": "d83ba294-506b-4e1e-909e-7074566da74d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 3.1897, -0.1859, -2.3211, -1.6164, -3.7888, -1.3246, -2.0342,\n",
              "           -0.1595, -3.3363, -0.2134,  0.7939, -3.1611, -3.9309, -4.2971,\n",
              "           -2.1758, -2.5180,  4.0481,  2.7661, -4.5270, -2.5663, -0.1953,\n",
              "           -1.5023,  2.7343,  0.6043,  3.5420, -2.7502,  2.0626, -1.0538,\n",
              "            0.6115, -1.6857, -3.4115, -2.4753, -3.5058, -3.4910, -3.0509,\n",
              "           -4.1252,  0.2435, -1.4038, -0.8380, -1.8344,  0.7848, -0.7693,\n",
              "           -0.2874, -2.8874,  3.3461, -4.0209, -1.3799, -1.3985, -0.0450,\n",
              "           -0.8859, -2.1166]]], grad_fn=<ViewBackward0>),\n",
              " None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r373yxYEo0Yu"
      },
      "source": [
        "## Language model\n",
        "- GPT is a LLM (large language models)\n",
        "- Language model encodes the statistical information of the language. It tells you what's likely to appear in a context.\n",
        "    - Example: (find 2 examples predicting next word / fill in the blanks)\n",
        "    - To train a language model, you feed it a lot of text (training data) so that it can learn the statistical information from it.\n",
        "- Word-level vs Character-level?\n",
        "- This notebook: Character-level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvp_S2yHuomF"
      },
      "source": [
        "\n",
        "## n-gram model\n",
        "### What this model do?\n",
        "- Predict how likely the next word is, given n-1 preceding words in the sequence.\n",
        "- For example,\n",
        "    - n=2 (bigram model)\n",
        "    - assume our dictionary has the following words: to, I, movies, like, watch, you, we, books\n",
        "    - input: `I like to watch ____`\n",
        "    - context sequence: `watch`\n",
        "    - what the bigram does, is that, it'll go over all words in the dictionary, and compute how likely it is the next word - more formally, `P(word_i|watch)`\n",
        "\n",
        "### How to train this model?\n",
        "yadayadayada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFP_M0VwKPm"
      },
      "source": [
        "#### Build the vocabulary\n",
        "- Some concepts in NLP\n",
        "- `Document`: text objects, which could be an article, a movie review, a passage or even a sentence.\n",
        "- `Corpus`: list of documents\n",
        "- `Vocabulary`: list of all the tokens in all documents. based on the task, token could be either a word, a character, or parts of the word (e.g. `playing` can be split into two tokens `play` and `ing`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fNfQe70syaaz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QgQEFj9LnSMM",
        "outputId": "9b43371e-4da7-4b30-bb79-b60cd62df136"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  artist                   song                                        link  \\\n",
              "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
              "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
              "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
              "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
              "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
              "\n",
              "                                                text  \n",
              "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
              "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
              "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
              "3  Making somebody happy is a question of give an...  \n",
              "4  Making somebody happy is a question of give an...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf857a59-0e75-4b39-b226-34a0870cd867\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Ahe's My Kind Of Girl</td>\n",
              "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
              "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Andante, Andante</td>\n",
              "      <td>/a/abba/andante+andante_20002708.html</td>\n",
              "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>As Good As New</td>\n",
              "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
              "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang</td>\n",
              "      <td>/a/abba/bang_20598415.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang-A-Boomerang</td>\n",
              "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf857a59-0e75-4b39-b226-34a0870cd867')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf857a59-0e75-4b39-b226-34a0870cd867 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf857a59-0e75-4b39-b226-34a0870cd867');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/spotify_millsongdata.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGzyejwYybYA",
        "outputId": "f1cdbd2e-db83-4a1b-bc72-152456c5f6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus has 57650 documents\n",
            "Sample document: look at her face, it's a wonderful face  \r\n",
            "and it means something special to me  \r\n",
            "look at the way that she smiles when she sees me  \r\n",
            "how lucky can one fellow be?  \r\n",
            "  \r\n",
            "she's just my kind of girl, she makes me feel fine  \r\n",
            "who could ever believe that she could be mine?  \r\n",
            "she's just my kind of girl, without her i'm blue  \r\n",
            "and if she ever leaves me what could i do, what could i do?  \r\n",
            "  \r\n",
            "and when we go for a walk in the park  \r\n",
            "and she holds me and squeezes my hand  \r\n",
            "we'll go on walking for hours and talking  \r\n",
            "about all the things that we plan  \r\n",
            "  \r\n",
            "she's just my kind of girl, she makes me feel fine  \r\n",
            "who could ever believe that she could be mine?  \r\n",
            "she's just my kind of girl, without her i'm blue  \r\n",
            "and if she ever leaves me what could i do, what could i do?\r\n",
            "\r\n",
            "\n"
          ]
        }
      ],
      "source": [
        "corpus = data.text.str.lower()\n",
        "sample_document = corpus[0]\n",
        "\n",
        "print(f'Corpus has {len(corpus)} documents')\n",
        "print(f'Sample document: {sample_document}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SneDZpVyu0u",
        "outputId": "2241197e-fee1-4199-d2c3-399c134c1f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocab: 51\n",
            "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "corpus_as_string = ' '.join(corpus.values)\n",
        "vocab = set(corpus_as_string)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'Length of vocab: {vocab_size}')\n",
        "print(sorted(list(vocab)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        # create a mapping from characters to integers\n",
        "        self.stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "\n",
        "    def encode(self, s):\n",
        "        # encoder: take a string, output a list of integers\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, l):\n",
        "        # decoder: take a list of integers, output a string\n",
        "        return ''.join([self.itos[i] for i in l])\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "print(tokenizer.encode(\"hii there\"))\n",
        "print(tokenizer.decode(tokenizer.encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQtWLjeY1xzw",
        "outputId": "c9fd3197-08da-4ec8-ed68-209227223bc8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 48, 48, 40, 41, 3, 43, 16, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRbFPwOh0AN7",
        "outputId": "3641bb93-71df-49ee-d45d-d16296577922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 48, 48, 40, 41, 3, 43, 16, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "ca2836e0-ac52-4500-dc75-d25694ba8ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([70426172]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(corpus_as_string), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "# print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EorJrz254GuS"
      },
      "source": [
        "#### Create dataset\n",
        "- train_size: 90%\n",
        "- val_size: 105\n",
        "- seq_len (block size for now): 8\n",
        "    - what is the maximum context length for predictions?\n",
        "- batch_size = 4 # how many independent sequences will we process in parallel?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.8*len(data)) # first 90% will be train, rest val\n",
        "\n",
        "train_ratio, val_ratio, test_ratio = 0.9, 0.05, 0.05\n",
        "\n",
        "train_size = int(train_ratio * len(data))\n",
        "val_size = int(val_ratio * len(data))\n",
        "test_size = int(test_ratio * len(data))\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size+val_size]\n",
        "test_data = data[-test_size:]\n",
        "\n",
        "assert len(train_data) == train_size\n",
        "assert len(val_data) == val_size\n",
        "assert len(test_data) == test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "c41d5b8d-4560-4157-eb0e-ee12692fa11c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15, 37, 37, 47, 40, 13, 41, 40,  3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "c14d69fc-ed74-45c7-9d59-90d6b88a6cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([15]) the target: 37\n",
            "when input is tensor([15, 37]) the target: 37\n",
            "when input is tensor([15, 37, 37]) the target: 47\n",
            "when input is tensor([15, 37, 37, 47]) the target: 40\n",
            "when input is tensor([15, 37, 37, 47, 40]) the target: 13\n",
            "when input is tensor([15, 37, 37, 47, 40, 13]) the target: 41\n",
            "when input is tensor([15, 37, 37, 47, 40, 13, 41]) the target: 40\n",
            "when input is tensor([15, 37, 37, 47, 40, 13, 41, 40]) the target: 3\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "c8385988-c6a2-48ba-cd8f-f18638324232"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[27, 43, 36, 40, 28, 37, 41, 40],\n",
              "         [29, 40, 30, 13, 28, 48, 27, 40],\n",
              "         [43, 40, 23, 13, 15, 15, 29, 40],\n",
              "         [40, 18, 37, 34, 16, 40, 13, 16]]),\n",
              " tensor([[43, 36, 40, 28, 37, 41, 40, 41],\n",
              "         [40, 30, 13, 28, 48, 27, 40, 20],\n",
              "         [40, 23, 13, 15, 15, 29, 40, 40],\n",
              "         [18, 37, 34, 16, 40, 13, 16, 30]]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split, batch_size, block_size):\n",
        "    '''\n",
        "    Generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    return: x, y\n",
        "        - x: (batch_size, block_size)\n",
        "        - y: (batch_size, block_size)\n",
        "    '''\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "get_batch('train', 4, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modeling: Simple BigramLanguageModel"
      ],
      "metadata": {
        "id": "j0EA_ByeJQO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nql_1ER53oCf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (batch_size,seq_len) tensor of integers\n",
        "\n",
        "        logits = self.token_embedding_table(idx) # (batch_size,seq_len,vocab_size)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "            # example: we have 2 classes [0, 1]\n",
        "            # logits = [[0.5, 0.5], [0.3, 0.7], [0.6, 0.4]]\n",
        "            # targets = [0, 1, 0]\n",
        "\n",
        "            logits = logits.view(batch_size*seq_len, vocab_size)\n",
        "            targets = targets.view(batch_size*seq_len)\n",
        "  \n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (batch_size, vocab_size)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_ppl(self, seq_tensor):\n",
        "        # Implemented from https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "        logits, _ = self(seq_tensor)    # logits = (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        logits = logits.view(batch_size*seq_len, vocab_size)\n",
        "        \n",
        "        logits = logits[:-1, :] # to compute P(x_i|x_(i-1))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        \n",
        "        ground_truths = seq_tensor.view(batch_size*seq_len)[1:]\n",
        "        ppl = probs[\n",
        "            torch.arange(batch_size*seq_len - 1),\n",
        "            ground_truths]\n",
        "\n",
        "        return torch.exp(ppl.mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LyricsGenerator:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def get_lyrics(self, start_phrase, max_new_tokens=2000):\n",
        "        start_phrase_as_ids = self.tokenizer.encode(start_phrase)\n",
        "        context = torch.tensor(start_phrase_as_ids, dtype=torch.long, device=self.device).reshape(1, -1)\n",
        "        output_tokens = self.model.generate(idx=context, max_new_tokens=max_new_tokens)[0].tolist()\n",
        "\n",
        "        return self.tokenizer.decode(output_tokens)"
      ],
      "metadata": {
        "id": "X8CyxCblHiHn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self,\n",
        "                 batch_size,\n",
        "                 max_iterations,\n",
        "                 eval_iterations,\n",
        "                 eval_intervals,\n",
        "                 lr,\n",
        "                 vocab_size,\n",
        "                 block_size,\n",
        "                 n_embd,\n",
        "                 n_head,\n",
        "                 n_layer,\n",
        "                 dropout):\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iterations = max_iterations\n",
        "        self.eval_iterations = eval_iterations\n",
        "        self.eval_intervals = eval_intervals\n",
        "\n",
        "        self.lr = lr\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.n_embd = n_embd\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def train(model_class, config: Config):\n",
        "    mlflow.set_experiment_tag('architecture', model_class.__name__)\n",
        "    mlflow.log_params(config.__dict__)\n",
        "\n",
        "    m = model_class(config.vocab_size)\n",
        "    m.to(config.device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(m.parameters(), lr=config.lr)\n",
        "\n",
        "    for step in range(config.max_iterations):\n",
        "        if step % config.eval_intervals == 0 or step == config.max_iterations - 1:\n",
        "            losses = estimate_loss(model)\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "            mlflow.log_metrics(losses, step=step)\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch(\n",
        "            'train',\n",
        "            batch_size=config.batch_size,\n",
        "            block_size=config.block_size)\n",
        "        xb, yb = xb.to(config.device), yb.to(config.device)\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = m(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    m.to(\"cpu\")\n",
        "    mlflow.pytorch.log_model(m, \"model\")\n",
        "\n",
        "    return m\n",
        "\n",
        "def evaluate_test(model, test_data, device):\n",
        "    model.eval()\n",
        "\n",
        "    model = model.to(device)\n",
        "    test_data = test_data.to(device)\n",
        "    ppl = model.evaluate_ppl(test_data.view(1, -1))\n",
        "\n",
        "    mlflow.log_metric('ppl', ppl)\n",
        "    return ppl\n"
      ],
      "metadata": {
        "id": "mJG_lKhlJd9R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import time"
      ],
      "metadata": {
        "id": "GdeTMhQj3pr6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modeling: Attention"
      ],
      "metadata": {
        "id": "BN4CZhZ_WSTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size, block_size)\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, config: Config):\n",
        "        super().__init__()\n",
        "\n",
        "        n_embd = config.n_embd\n",
        "        block_size = config.block_size\n",
        "        dropout = config.dropout\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "    \n",
        "        n_head = config.n_head\n",
        "        n_embd = config.n_embd\n",
        "        dropout = config.dropout\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size, config) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "\n",
        "        n_embd = config.n_embd\n",
        "        n_head = config.n_head\n",
        "        dropout = config.dropout\n",
        "        \n",
        "        self.sa = MultiHeadAttention(config)\n",
        "        self.ffwd = FeedFoward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModelAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_ppl(self, seq_tensor):\n",
        "        _, test_size = seq_tensor.shape\n",
        "        num_chunks = test_size // block_size\n",
        "\n",
        "        # update test size to fit with block_size\n",
        "        test_size = num_chunks * block_size\n",
        "        seq_tensor = seq_tensor[:, :test_size]\n",
        "\n",
        "        # split seq_tensor into chunks of blocksize\n",
        "        test_input = seq_tensor.view(num_chunks, block_size)\n",
        "\n",
        "        # Implemented from https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "        logits, _ = self(test_input)    # logits = (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        logits = logits.view(test_size, vocab_size)\n",
        "        logits = logits[:-1, :] # to compute P(x_i|x_(i-1))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        \n",
        "        # TODO: write docs to explain this part\n",
        "        ground_truths = test_input.view(test_size)[1:]\n",
        "        ppl = probs[\n",
        "            torch.arange(test_size - 1),\n",
        "            ground_truths]\n",
        "\n",
        "        return torch.exp(ppl.mean())\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "ayuMVxMw2U5p"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(\n",
        "    batch_size = 16, # how many independent sequences will we process in parallel?\n",
        "    block_size = 32, # what is the maximum context length for predictions?\n",
        "    max_iterations = 1000,\n",
        "    eval_intervals = 50,\n",
        "    lr = 1e-3,\n",
        "    eval_iterations = 200,\n",
        "    n_embd = 64,\n",
        "    n_head = 4,\n",
        "    n_layer = 4,\n",
        "    dropout = 0.0,\n",
        "    vocab_size=vocab_size,\n",
        ")"
      ],
      "metadata": {
        "id": "vtacs8JxLUuS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = data[-test_size:]\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "tokenizer_path = 'tokenizer.pkl'\n",
        "\n",
        "with open(tokenizer_path, 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "with mlflow.start_run(run_name=str(time.time())):\n",
        "    model = train(BigramLanguageModelAttention, config)\n",
        "    evaluate_test(model, test_data, config.device)\n",
        "\n",
        "    mlflow.log_artifact(tokenizer_path, \"model/artifacts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "FtwMJ5uEQDUl",
        "outputId": "eb722457-2da6-4973-e665-502ee5887450"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c0959212139a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBigramLanguageModelAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mevaluate_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b7bddaa89a65>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_class, config)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-1bc77bb5ef65>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# each token directly reads off the logits for the next token from a lookup table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'vocab_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import time\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "tokenizer_path = 'tokenizer.pkl'\n",
        "\n",
        "with open(tokenizer_path, 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "with mlflow.start_run(run_name=str(time.time())):\n",
        "    model_class = BigramLanguageModelAttention\n",
        "    mlflow.set_experiment_tag('architecture', model_class.__name__)\n",
        "\n",
        "    model = model_class(config)\n",
        "    model = model.to(config.device)\n",
        "    # print the number of parameters in the model\n",
        "    print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "    # create a PyTorch optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss(model)\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch('train', batch_size, block_size)\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model = model.to(\"cpu\")\n",
        "    mlflow.pytorch.log_model(model, \"model\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    test_data = data[-test_size:]\n",
        "    evaluate_test(model, test_data)\n",
        "\n",
        "    mlflow.log_artifact(tokenizer_path, \"model/artifacts\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L049NSmajX2P",
        "outputId": "6f2ff26b-24eb-4637-a5b5-2f2b755b4661"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.207923 M parameters\n",
            "step 0: train loss 3.9961, val loss 3.9921\n",
            "step 50: train loss 2.5805, val loss 2.5684\n",
            "step 100: train loss 2.4035, val loss 2.3977\n",
            "step 150: train loss 2.3225, val loss 2.3069\n",
            "step 200: train loss 2.2434, val loss 2.2290\n",
            "step 250: train loss 2.1775, val loss 2.1517\n",
            "step 300: train loss 2.1186, val loss 2.1051\n",
            "step 350: train loss 2.0836, val loss 2.0693\n",
            "step 400: train loss 2.0440, val loss 2.0388\n",
            "step 450: train loss 2.0284, val loss 2.0165\n",
            "step 500: train loss 1.9969, val loss 1.9777\n",
            "step 550: train loss 1.9553, val loss 1.9358\n",
            "step 600: train loss 1.9148, val loss 1.9188\n",
            "step 650: train loss 1.9041, val loss 1.9007\n",
            "step 700: train loss 1.8807, val loss 1.8627\n",
            "step 750: train loss 1.8715, val loss 1.8550\n",
            "step 800: train loss 1.8383, val loss 1.8385\n",
            "step 850: train loss 1.8283, val loss 1.8216\n",
            "step 900: train loss 1.8187, val loss 1.8021\n",
            "step 950: train loss 1.8064, val loss 1.7924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/05/20 08:06:21 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 999: train loss 1.7845, val loss 1.7776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/05/20 08:06:26 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5ZPLxOGMwiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ARCHIVE"
      ],
      "metadata": {
        "id": "msXMNibsWe34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs4kI8YdEkQj"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "num_iterations = 1000\n",
        "lr=1e-3\n",
        "\n",
        "config = Config(\n",
        "    batch_size=batch_size,\n",
        "    num_iterations=num_iterations,\n",
        "    lr=lr,\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=block_size\n",
        ")\n",
        "\n",
        "test_data = data[-test_size:]\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "tokenizer_path = 'tokenizer.pkl'\n",
        "\n",
        "with open(tokenizer_path, 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "with mlflow.start_run(run_name=str(time.time())):\n",
        "    model = train(BigramLanguageModel, config)\n",
        "    evaluate_test(model, test_data)\n",
        "\n",
        "    mlflow.log_artifact(tokenizer_path, \"model/artifacts\")\n",
        "\n",
        "# lyrics_gen = LyricsGenerator(model, tokenizer)\n",
        "# lyrics_gen.get_lyrics(\"last christmas i gave you my heart\", 500)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "msXMNibsWe34"
      ],
      "mount_file_id": "11v-z8aSyolKHJBarefSI1rZO_KGSlOCL",
      "authorship_tag": "ABX9TyPpe7elJ571OenjoDNYk7Zu"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}