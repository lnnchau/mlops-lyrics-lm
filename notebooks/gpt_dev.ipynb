{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqSIiWsRKdi7"
      },
      "source": [
        "# Configure MLflow 🧐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdco6DJEr1Hs"
      },
      "source": [
        "import requests\n",
        "from getpass import getpass\n",
        "import datetime"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScoLtmkOSTLE"
      },
      "source": [
        "**Set Environment Variables**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEOF1GMt9pIa"
      },
      "source": [
        "**Initialize MLflow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKazlYv0rKoC"
      },
      "source": [
        "!pip install mlflow --quiet\n",
        "\n",
        "import mlflow\n",
        "import os"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVIOWPHaaX8I"
      },
      "source": [
        "**Set Local Configurations**\n",
        "\n",
        "Under the [Token tab](https://dagshub.com/user/settings/tokens) in the user setting, copy the default token and use it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7c3I6TCoNbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6ca083-b57a-4528-9eb1-e51c2e11a39f"
      },
      "source": [
        "REPO_OWNER = input('Repo owner: ').strip()\n",
        "REPO_NAME = input('Repo name:').strip()\n",
        "USER_NAME = REPO_OWNER\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = USER_NAME\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token: ')\n",
        "\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/{REPO_OWNER}/{REPO_NAME}.mlflow')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo owner: lnnchau\n",
            "Repo name:mlops-lyrics-lm\n",
            "Enter your DAGsHub access token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg5u4G9tvL_d"
      },
      "source": [
        "### Source\n",
        "- https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "- https://huyenchip.com/2023/05/02/rlhf.html#language_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r373yxYEo0Yu"
      },
      "source": [
        "## Language model\n",
        "- GPT is a LLM (large language models)\n",
        "- Language model encodes the statistical information of the language. It tells you what's likely to appear in a context.\n",
        "    - Example: (find 2 examples predicting next word / fill in the blanks)\n",
        "    - To train a language model, you feed it a lot of text (training data) so that it can learn the statistical information from it.\n",
        "- Word-level vs Character-level?\n",
        "- This notebook: Character-level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvp_S2yHuomF"
      },
      "source": [
        "\n",
        "## n-gram model\n",
        "### What this model do?\n",
        "- Predict how likely the next word is, given n-1 preceding words in the sequence.\n",
        "- For example,\n",
        "    - n=2 (bigram model)\n",
        "    - assume our dictionary has the following words: to, I, movies, like, watch, you, we, books\n",
        "    - input: `I like to watch ____`\n",
        "    - context sequence: `watch`\n",
        "    - what the bigram does, is that, it'll go over all words in the dictionary, and compute how likely it is the next word - more formally, `P(word_i|watch)`\n",
        "\n",
        "### How to train this model?\n",
        "yadayadayada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFP_M0VwKPm"
      },
      "source": [
        "#### Build the vocabulary\n",
        "- Some concepts in NLP\n",
        "- `Document`: text objects, which could be an article, a movie review, a passage or even a sentence.\n",
        "- `Corpus`: list of documents\n",
        "- `Vocabulary`: list of all the tokens in all documents. based on the task, token could be either a word, a character, or parts of the word (e.g. `playing` can be split into two tokens `play` and `ing`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "fNfQe70syaaz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QgQEFj9LnSMM",
        "outputId": "801e8d94-3511-42a0-fe95-087b5576f1c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  artist                   song                                        link  \\\n",
              "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
              "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
              "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
              "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
              "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
              "\n",
              "                                                text  \n",
              "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
              "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
              "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
              "3  Making somebody happy is a question of give an...  \n",
              "4  Making somebody happy is a question of give an...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15ce3b4d-70b1-4319-bdc4-d0feab213b27\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Ahe's My Kind Of Girl</td>\n",
              "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
              "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Andante, Andante</td>\n",
              "      <td>/a/abba/andante+andante_20002708.html</td>\n",
              "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>As Good As New</td>\n",
              "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
              "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang</td>\n",
              "      <td>/a/abba/bang_20598415.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang-A-Boomerang</td>\n",
              "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15ce3b4d-70b1-4319-bdc4-d0feab213b27')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15ce3b4d-70b1-4319-bdc4-d0feab213b27 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15ce3b4d-70b1-4319-bdc4-d0feab213b27');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/spotify_millsongdata.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGzyejwYybYA",
        "outputId": "9bc8e5ea-f233-4e4e-ceaa-84075feb0e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus has 57650 documents\n",
            "Sample document: look at her face, it's a wonderful face  \r\n",
            "and it means something special to me  \r\n",
            "look at the way that she smiles when she sees me  \r\n",
            "how lucky can one fellow be?  \r\n",
            "  \r\n",
            "she's just my kind of girl, she makes me feel fine  \r\n",
            "who could ever believe that she could be mine?  \r\n",
            "she's just my kind of girl, without her i'm blue  \r\n",
            "and if she ever leaves me what could i do, what could i do?  \r\n",
            "  \r\n",
            "and when we go for a walk in the park  \r\n",
            "and she holds me and squeezes my hand  \r\n",
            "we'll go on walking for hours and talking  \r\n",
            "about all the things that we plan  \r\n",
            "  \r\n",
            "she's just my kind of girl, she makes me feel fine  \r\n",
            "who could ever believe that she could be mine?  \r\n",
            "she's just my kind of girl, without her i'm blue  \r\n",
            "and if she ever leaves me what could i do, what could i do?\r\n",
            "\r\n",
            "\n"
          ]
        }
      ],
      "source": [
        "corpus = data.text.str.lower()\n",
        "sample_document = corpus[0]\n",
        "\n",
        "print(f'Corpus has {len(corpus)} documents')\n",
        "print(f'Sample document: {sample_document}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SneDZpVyu0u",
        "outputId": "0145cfe5-662b-4743-b8b7-79f9ce4c5438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocab: 51\n",
            "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "corpus_as_string = ' '.join(corpus.values)\n",
        "vocab = set(corpus_as_string)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'Length of vocab: {vocab_size}')\n",
        "print(sorted(list(vocab)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRbFPwOh0AN7",
        "outputId": "8b889412-abc7-4923-cf48-5c9b26c42bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 2, 2, 24, 45, 11, 46, 44, 46]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "39418f59-af46-4fb1-fe8d-8a3508c75d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([70426172]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(corpus_as_string), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "# print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EorJrz254GuS"
      },
      "source": [
        "#### Create dataset\n",
        "- train_size: 90%\n",
        "- val_size: 105\n",
        "- seq_len (block size for now): 8\n",
        "    - what is the maximum context length for predictions?\n",
        "- batch_size = 4 # how many independent sequences will we process in parallel?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.8*len(data)) # first 90% will be train, rest val\n",
        "\n",
        "train_ratio, val_ratio, test_ratio = 0.8, 0.1, 0.1\n",
        "\n",
        "train_size = int(train_ratio * len(data))\n",
        "val_size = int(val_ratio * len(data))\n",
        "test_size = int(test_ratio * len(data))\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size+val_size]\n",
        "test_data = data[-test_size:]\n",
        "\n",
        "assert len(train_data) == train_size\n",
        "assert len(val_data) == val_size\n",
        "assert len(test_data) == test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "be3cfc28-9c47-4e51-8a92-7cdda81b4129"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([30, 19, 19, 35, 24, 16, 45, 24, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "7b4379ca-eef4-40fb-f1cb-2df32543cd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([30]) the target: 19\n",
            "when input is tensor([30, 19]) the target: 19\n",
            "when input is tensor([30, 19, 19]) the target: 35\n",
            "when input is tensor([30, 19, 19, 35]) the target: 24\n",
            "when input is tensor([30, 19, 19, 35, 24]) the target: 16\n",
            "when input is tensor([30, 19, 19, 35, 24, 16]) the target: 45\n",
            "when input is tensor([30, 19, 19, 35, 24, 16, 45]) the target: 24\n",
            "when input is tensor([30, 19, 19, 35, 24, 16, 45, 24]) the target: 11\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "7f1f47da-1546-4ff9-bdd6-7bfbda0623bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[17, 16, 17, 16, 24, 24, 32, 26],\n",
              "         [24, 17, 13, 24, 37,  2,  1, 16],\n",
              "         [26, 24, 24, 32, 26, 45, 11, 46],\n",
              "         [24, 38, 13, 24, 24, 32, 26,  8]]),\n",
              " tensor([[16, 17, 16, 24, 24, 32, 26, 38],\n",
              "         [17, 13, 24, 37,  2,  1, 16, 30],\n",
              "         [24, 24, 32, 26, 45, 11, 46,  1],\n",
              "         [38, 13, 24, 24, 32, 26,  8, 16]]))"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split, batch_size, block_size):\n",
        "    '''\n",
        "    Generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    return: x, y\n",
        "        - x: (batch_size, block_size)\n",
        "        - y: (batch_size, block_size)\n",
        "    '''\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "get_batch('train', 4, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modeling: Simple BigramLanguageModel"
      ],
      "metadata": {
        "id": "j0EA_ByeJQO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "nql_1ER53oCf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (batch_size,seq_len) tensor of integers\n",
        "\n",
        "        logits = self.token_embedding_table(idx) # (batch_size,seq_len,vocab_size)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "            # example: we have 2 classes [0, 1]\n",
        "            # logits = [[0.5, 0.5], [0.3, 0.7], [0.6, 0.4]]\n",
        "            # targets = [0, 1, 0]\n",
        "\n",
        "            logits = logits.view(batch_size*seq_len, vocab_size)\n",
        "            targets = targets.view(batch_size*seq_len)\n",
        "  \n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (batch_size, vocab_size)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (batch_size, vocab_size)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_ppl(self, seq_tensor):\n",
        "        # Implemented from https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "        logits, _ = self(seq_tensor)    # logits = (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        logits = logits.view(batch_size*seq_len, vocab_size)\n",
        "        \n",
        "        logits = logits[:-1, :] # to compute P(x_i|x_(i-1))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        \n",
        "        ground_truths = seq_tensor.view(batch_size*seq_len)[1:]\n",
        "        ppl = probs[\n",
        "            torch.arange(batch_size*seq_len - 1),\n",
        "            ground_truths]\n",
        "\n",
        "        return torch.exp(ppl.mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LyricsGenerator:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def get_lyrics(self, start_phrase, max_new_tokens=2000):\n",
        "        context = torch.tensor(encode(start_phrase), dtype=torch.long, device=self.device).reshape(1, -1)\n",
        "        output_tokens = self.model.generate(idx=context, max_new_tokens=max_new_tokens)[0].tolist()\n",
        "\n",
        "        return decode(output_tokens)"
      ],
      "metadata": {
        "id": "X8CyxCblHiHn"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self,\n",
        "                 batch_size,\n",
        "                 num_iterations,\n",
        "                 lr,\n",
        "                 vocab_size,\n",
        "                 block_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_iterations = num_iterations\n",
        "        self.lr = lr\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def train(model_class, config: Config):\n",
        "    mlflow.set_experiment_tag('architecture', model_class.__name__)\n",
        "    mlflow.log_params(config.__dict__)\n",
        "\n",
        "    m = model_class(config.vocab_size)\n",
        "    m.to(config.device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(m.parameters(), lr=config.lr)\n",
        "\n",
        "    for step in range(config.num_iterations):\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch(\n",
        "            'train',\n",
        "            batch_size=config.batch_size,\n",
        "            block_size=config.block_size)\n",
        "        xb, yb = xb.to(config.device), yb.to(config.device)\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = m(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mlflow.log_metric('loss', loss.item(), step=step)\n",
        "\n",
        "    mlflow.pytorch.log_model(m, \"model\", registered_model_name=model_class.__name__)\n",
        "\n",
        "    return m\n",
        "\n",
        "def evaluate_test(model, test_data):\n",
        "    model.eval()\n",
        "    model.cpu()\n",
        "    ppl = model.evaluate_ppl(test_data.view(1, -1))\n",
        "\n",
        "    mlflow.log_metric('ppl', ppl)\n",
        "    return ppl\n"
      ],
      "metadata": {
        "id": "mJG_lKhlJd9R"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "d038f3b7-4047-45d7-97e5-2c23beac8181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/05/06 04:00:06 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.0+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2023/05/06 04:00:10 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.0+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "Registered model 'BigramLanguageModel' already exists. Creating a new version of this model...\n",
            "2023/05/06 04:00:20 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: BigramLanguageModel, version 3\n",
            "Created version '3' of model 'BigramLanguageModel'.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "num_iterations = 100\n",
        "lr=1e-3\n",
        "\n",
        "config = Config(\n",
        "    batch_size=batch_size,\n",
        "    num_iterations=num_iterations,\n",
        "    lr=lr,\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=block_size\n",
        ")\n",
        "\n",
        "test_data = data[-test_size:]\n",
        "\n",
        "import time\n",
        "\n",
        "with mlflow.start_run(run_name=str(time.time())):\n",
        "    model = train(BigramLanguageModel, config)\n",
        "    evaluate_test(model, test_data)\n",
        "\n",
        "# lyrics_gen = LyricsGenerator(model)\n",
        "# lyrics_gen.get_lyrics(\"last christmas i gave you my heart\", 500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "11Vyf4yMLEyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modeling: Attention"
      ],
      "metadata": {
        "id": "BN4CZhZ_WSTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size, block_size)\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModelAttention(BigramLanguageModel):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(vocab_size)\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModelAttention()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', batch_size, block_size)\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "ayuMVxMw2U5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = data[-test_size:]\n",
        "num_chunks = test_size // block_size\n",
        "chunks = [test_data[i:i+block_size] for i in range(0, num_chunks, block_size)]\n",
        "test_input = torch.stack(chunks)\n",
        "test_input.shape"
      ],
      "metadata": {
        "id": "5AxPz4gBMi5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute ppl\n",
        "ppl = m.evaluate_ppl(test_input.to(device))\n",
        "print(ppl)\n",
        "\n",
        "# # generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "lTRtx3Y_O-QF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ykFP_M0VwKPm",
        "BN4CZhZ_WSTl"
      ],
      "gpuType": "T4",
      "mount_file_id": "11v-z8aSyolKHJBarefSI1rZO_KGSlOCL",
      "authorship_tag": "ABX9TyNJVR6v9hURLZjVoMITM7oD"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}